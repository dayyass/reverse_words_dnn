{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Counter as CounterType, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "\n",
    "def set_global_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Set global seed for reproducibility.\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_global_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "config = {\n",
    "    \"BATCH_SIZE\":    256,\n",
    "    \"LEARNING_RATE\": 1e-4,\n",
    "    \"N_EPOCHS\":      10,\n",
    "\n",
    "    \"EMBEDDING_DIM\":         100,\n",
    "    \"ENCODER_HIDDEN_SIZE\":   128,\n",
    "    \"ENCODER_NUM_LAYERS\":    1,\n",
    "    \"ENCODER_DROPOUT\":       0.0,\n",
    "    \"ENCODER_BIDIRECTIONAL\": True,\n",
    "    \"DECODER_NUM_LAYERS\":    1,\n",
    "    \"DECODER_DROPOUT\":       0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "\n",
    "experiment_name = f\"Seq2SeqLSTM_BATCH_{config['BATCH_SIZE']}_LR_{config['LEARNING_RATE']}_N_EPOCHS_{config['N_EPOCHS']}\"\n",
    "\n",
    "writer = SummaryWriter(\n",
    "    log_dir=f\"runs/tmp\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preapre Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        path_to_data: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.path_to_data = path_to_data\n",
    "        self.dataset = self._prepare_dataset(path_to_data)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "    ) -> List[str]:\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_dataset(path_to_data: str) -> List[List[str]]:\n",
    "        dataset = []\n",
    "\n",
    "        pos_dir = os.path.join(path_to_data, \"pos\")\n",
    "        neg_dir = os.path.join(path_to_data, \"neg\")\n",
    "        \n",
    "        for dir in [pos_dir, neg_dir]:\n",
    "            for filename in tqdm(\n",
    "                os.listdir(dir),\n",
    "                desc=\"parse txt files\",\n",
    "            ):\n",
    "                if not filename.endswith(\".txt\"):\n",
    "                    continue\n",
    "                with open(os.path.join(dir, filename), mode=\"r\") as fp:\n",
    "                    review = fp.read()\n",
    "                    dataset.append(word_tokenize(review.lower()))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(path_to_data=\"data/aclImdb/train\")\n",
    "test_dataset = IMDBDataset(path_to_data=\"data/aclImdb/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_len_distr = [len(review) for review in train_dataset]\n",
    "test_dataset_len_distr = [len(review) for review in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    train_dataset_len_distr,\n",
    "    bins=len(set(train_dataset_len_distr)),\n",
    "    alpha=0.5,\n",
    "    label=\"train\",\n",
    ")\n",
    "plt.hist(\n",
    "    test_dataset_len_distr,\n",
    "    bins=len(set(test_dataset_len_distr)),\n",
    "    alpha=0.5,\n",
    "    label=\"test\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.title(\"Review's length distribution\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_n_digits_distr = Counter([len(str(length)) for length in train_dataset_len_distr])\n",
    "train_dataset_n_digits_distr.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_n_digits_distr = Counter([len(str(length)) for length in test_dataset_len_distr])\n",
    "test_dataset_n_digits_distr.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_counter = Counter()\n",
    "\n",
    "for review in train_dataset:\n",
    "    tokens_counter.update(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens_counter), tokens_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token2Idx:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        tokens_counter: CounterType,\n",
    "        min_df: int,\n",
    "    ):\n",
    "        self.tokens_counter = tokens_counter\n",
    "        self.min_df = min_df\n",
    "\n",
    "        self.token2idx = self._prepare_token2idx(\n",
    "            tokens_counter=tokens_counter,\n",
    "            min_df=min_df,\n",
    "        )\n",
    "    \n",
    "    def __call__(\n",
    "        self,\n",
    "        seq: List[str],\n",
    "    ) -> torch.LongTensor:\n",
    "        return [self.token2idx.get(token, self.token2idx[\"<unk>\"]) for token in seq]\n",
    "    \n",
    "    def __getitem__(self, key: str) -> int:\n",
    "        return self.token2idx[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_token2idx(\n",
    "        tokens_counter: CounterType,\n",
    "        min_df: int,\n",
    "    ) -> Dict[str, int]:\n",
    "        token2idx = {\n",
    "            \"<bos>\": 0,\n",
    "            \"<eos>\": 1,\n",
    "            \"<unk>\": 2,\n",
    "            \"<pad>\": 3,\n",
    "        }\n",
    "\n",
    "        for token, cnt in tqdm(\n",
    "            tokens_counter.most_common(),\n",
    "            desc=\"loop over unique tokens\",\n",
    "        ):\n",
    "            if token in token2idx:\n",
    "                continue\n",
    "            if cnt < min_df:\n",
    "                continue\n",
    "\n",
    "            token2idx[token] = len(token2idx)\n",
    "        \n",
    "        return token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = Token2Idx(\n",
    "    tokens_counter=tokens_counter,\n",
    "    min_df=5,  # hardcoded\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token2idx.token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx(train_dataset[0])[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [train_dataset, test_dataset]:\n",
    "    for review in tqdm(\n",
    "        dataset,\n",
    "        desc=\"assertion loop\",\n",
    "    ):\n",
    "        assert len(review) == len(token2idx(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        token2idx: Token2Idx,\n",
    "    ):\n",
    "        self.token2idx = token2idx\n",
    "    \n",
    "    def __call__(\n",
    "        self,\n",
    "        batch: List[List[str]],\n",
    "    ) -> torch.LongTensor:\n",
    "        tensor_seq = []\n",
    "        tensor_inv_seq = []\n",
    "        for seq in batch:\n",
    "            tokenized_seq = self.token2idx(seq)\n",
    "            tensor_seq.append(torch.LongTensor(tokenized_seq))\n",
    "\n",
    "            tokenized_inv_seq = [self.token2idx[\"<bos>\"]] + tokenized_seq[::-1] + [self.token2idx[\"<eos>\"]]\n",
    "            tensor_inv_seq.append(torch.LongTensor(tokenized_inv_seq))\n",
    "\n",
    "        padded_sequences =  torch.nn.utils.rnn.pad_sequence(\n",
    "            sequences=tensor_seq,\n",
    "            batch_first=True,\n",
    "            padding_value=self.token2idx[\"<pad>\"],\n",
    "        )\n",
    "        padded_inv_sequences =  torch.nn.utils.rnn.pad_sequence(\n",
    "            sequences=tensor_inv_seq,\n",
    "            batch_first=True,\n",
    "            padding_value=self.token2idx[\"<pad>\"],\n",
    "        )\n",
    "        return padded_sequences, padded_inv_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = Collator(token2idx=token2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=config[\"BATCH_SIZE\"],\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, inv_seq = next(iter(train_dataloader))\n",
    "seq.shape, inv_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, inv_seq = next(iter(test_dataloader))\n",
    "seq.shape, inv_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_parameters(model: torch.nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqLSTM(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        encoder_hidden_size: int,\n",
    "        encoder_num_layers: int,\n",
    "        encoder_dropout: float,\n",
    "        encoder_bidirectional: bool,\n",
    "        decoder_num_layers: int,\n",
    "        decoder_dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            num_embeddings=num_embeddings,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=token2idx[\"<pad>\"],\n",
    "\n",
    "        )\n",
    "        self.encoder = torch.nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=encoder_hidden_size,\n",
    "            num_layers=encoder_num_layers,\n",
    "            dropout=encoder_dropout,\n",
    "            bidirectional=encoder_bidirectional,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        decoder_hidden_size = encoder_hidden_size * (2 if encoder_bidirectional else 1)\n",
    "        self.decoder = torch.nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=decoder_hidden_size,\n",
    "            num_layers=decoder_num_layers,\n",
    "            dropout=decoder_dropout,\n",
    "            bidirectional=False,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.head = torch.nn.Linear(\n",
    "            in_features=decoder_hidden_size,\n",
    "            out_features=num_embeddings,\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        seq: torch.LongTensor,\n",
    "        inv_seq: torch.LongTensor,\n",
    "    ):\n",
    "        emb = self._embed(seq)\n",
    "        encoder_output, _ = self.encoder(emb)\n",
    "        skip_thoughts = self._get_skip_thoughts(encoder_output=encoder_output)\n",
    "\n",
    "        inv_emb = self._embed(inv_seq)\n",
    "        decoder_output, _ = self.decoder(inv_emb, (skip_thoughts, skip_thoughts))\n",
    "\n",
    "        decoder_output, _ = self._pad_packed_sequence(sequence=decoder_output)\n",
    "        logits = self.head(decoder_output)\n",
    "        return logits\n",
    "    \n",
    "    def _embed(\n",
    "        self,\n",
    "        seq: torch.LongTensor,\n",
    "    ) -> torch.nn.utils.rnn.PackedSequence:\n",
    "        emb = self.embedding(seq)\n",
    "        lengths = (seq != token2idx[\"<pad>\"]).sum(dim=1)\n",
    "        return torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            input=emb, lengths=lengths,\n",
    "            batch_first=True, enforce_sorted=False,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _pad_packed_sequence(\n",
    "        sequence: torch.nn.utils.rnn.PackedSequence,\n",
    "    ) -> Tuple[torch.Tensor, torch.LongTensor]:\n",
    "        return torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=sequence,\n",
    "            batch_first=True,\n",
    "            padding_value=token2idx[\"<pad>\"],\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_skip_thoughts(\n",
    "        encoder_output: torch.nn.utils.rnn.PackedSequence,\n",
    "    ) -> torch.Tensor:\n",
    "        encoder_output, lengths = Seq2SeqLSTM._pad_packed_sequence(sequence=encoder_output)\n",
    "        return torch.index_select(\n",
    "            input=encoder_output,\n",
    "            dim=1,\n",
    "            index=lengths - 1,\n",
    "        ).mean(dim=1).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqLSTM(\n",
    "    num_embeddings=len(token2idx.token2idx),\n",
    "    embedding_dim=config[\"EMBEDDING_DIM\"],\n",
    "    encoder_hidden_size=config[\"ENCODER_HIDDEN_SIZE\"],\n",
    "    encoder_num_layers=config[\"ENCODER_NUM_LAYERS\"],\n",
    "    encoder_dropout=config[\"ENCODER_DROPOUT\"],\n",
    "    encoder_bidirectional=config[\"ENCODER_BIDIRECTIONAL\"],\n",
    "    decoder_num_layers=config[\"DECODER_NUM_LAYERS\"],\n",
    "    decoder_dropout=config[\"DECODER_DROPOUT\"],\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, inv_seq = next(iter(train_dataloader))\n",
    "seq.to(device), inv_seq.to(device)\n",
    "seq.shape, inv_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(seq, inv_seq)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config[\"LEARNING_RATE\"],\n",
    ")\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(output.transpose(1, 2), inv_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: Seq2SeqLSTM,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    writer: SummaryWriter,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    One training cycle (loop).\n",
    "    \n",
    "    Args:\n",
    "        model (Seq2SeqLSTM): model.\n",
    "        dataloader (torch.utils.data.DataLoader): dataloader.\n",
    "        optimizer (torch.optim.Optimizer): optimizer.\n",
    "        criterion (torch.nn.Module): criterion.\n",
    "        writer (SummaryWriter): tensorboard writer.\n",
    "        device (torch.device): cpu or cuda.\n",
    "        epoch (int): number of current epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = []\n",
    "    batch_metrics_list = defaultdict(list)\n",
    "\n",
    "    for i, (seq, inv_seq) in tqdm(\n",
    "        enumerate(dataloader),\n",
    "        total=len(dataloader),\n",
    "        desc=\"loop over train batches\",\n",
    "    ):\n",
    "\n",
    "        seq, inv_seq = seq.to(device), inv_seq.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        scores = model(seq, inv_seq)\n",
    "        loss = criterion(scores.transpose(1, 2), inv_seq)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "        writer.add_scalar(\n",
    "            \"batch loss / train\", loss.item(), epoch * len(dataloader) + i\n",
    "        )\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     model.eval()\n",
    "        #     scores_inference = model(seq, inv_seq)\n",
    "        #     model.train()\n",
    "\n",
    "        # batch_metrics = compute_metrics(\n",
    "        #     outputs=scores_inference,\n",
    "        #     targets=tgt,\n",
    "        # )\n",
    "\n",
    "        # for metric_name, metric_value in batch_metrics.items():\n",
    "        #     batch_metrics_list[metric_name].append(metric_value)\n",
    "        #     writer.add_scalar(\n",
    "        #         f\"batch {metric_name} / train\",\n",
    "        #         metric_value,\n",
    "        #         epoch * len(dataloader) + i,\n",
    "        #     )\n",
    "\n",
    "    avg_loss = np.mean(epoch_loss)\n",
    "    print(f\"Train loss: {avg_loss}\\n\")\n",
    "    writer.add_scalar(\"loss / train\", avg_loss, epoch)\n",
    "\n",
    "    for metric_name, metric_value_list in batch_metrics_list.items():\n",
    "        metric_value = np.mean(metric_value_list)\n",
    "        print(f\"Train {metric_name}: {metric_value}\\n\")\n",
    "        writer.add_scalar(f\"{metric_name} / train\", metric_value, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(\n",
    "    model: Seq2SeqLSTM,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    criterion: torch.nn.Module,\n",
    "    writer: SummaryWriter,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    One evaluation cycle (loop).\n",
    "\n",
    "    Args:\n",
    "        model (Seq2SeqLSTM): model.\n",
    "        dataloader (torch.utils.data.DataLoader): dataloader.\n",
    "        criterion (torch.nn.Module): criterion.\n",
    "        writer (SummaryWriter): tensorboard writer.\n",
    "        device (torch.device): cpu or cuda.\n",
    "        epoch (int): number of current epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = []\n",
    "    batch_metrics_list = defaultdict(list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (seq, inv_seq) in tqdm(\n",
    "            enumerate(dataloader),\n",
    "            total=len(dataloader),\n",
    "            desc=\"loop over test batches\",\n",
    "        ):\n",
    "\n",
    "            seq, inv_seq = seq.to(device), inv_seq.to(device)\n",
    "\n",
    "            scores = model(seq, inv_seq)\n",
    "            loss = criterion(scores.transpose(1, 2), inv_seq)\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            writer.add_scalar(\n",
    "                \"batch loss / test\", loss.item(), epoch * len(dataloader) + i\n",
    "            )\n",
    "\n",
    "            # batch_metrics = compute_metrics(\n",
    "            #     outputs=scores,\n",
    "            #     targets=tgt,\n",
    "            # )\n",
    "\n",
    "            # for metric_name, metric_value in batch_metrics.items():\n",
    "            #     batch_metrics_list[metric_name].append(metric_value)\n",
    "            #     writer.add_scalar(\n",
    "            #         f\"batch {metric_name} / test\",\n",
    "            #         metric_value,\n",
    "            #         epoch * len(dataloader) + i,\n",
    "            #     )\n",
    "\n",
    "        avg_loss = np.mean(epoch_loss)\n",
    "        print(f\"Test loss:  {avg_loss}\\n\")\n",
    "        writer.add_scalar(\"loss / test\", avg_loss, epoch)\n",
    "\n",
    "        for metric_name, metric_value_list in batch_metrics_list.items():\n",
    "            metric_value = np.mean(metric_value_list)\n",
    "            print(f\"Test {metric_name}: {metric_value}\\n\")\n",
    "            writer.add_scalar(f\"{metric_name} / test\", metric_value, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    n_epochs: int,\n",
    "    model: Seq2SeqLSTM,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    writer: SummaryWriter,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Training loop.\n",
    "    \n",
    "    Args:\n",
    "        n_epochs (int): number of epochs to train.\n",
    "        model (Seq2SeqLSTM): model.\n",
    "        train_dataloader (torch.utils.data.DataLoader): train_dataloader.\n",
    "        test_dataloader (torch.utils.data.DataLoader): test_dataloader.\n",
    "        optimizer (torch.optim.Optimizer): optimizer.\n",
    "        criterion (torch.nn.Module): criterion.\n",
    "        writer (SummaryWriter): tensorboard writer.\n",
    "        device (torch.device): cpu or cuda.\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        print(f\"Epoch [{epoch+1} / {n_epochs}]\\n\")\n",
    "\n",
    "        train_epoch(\n",
    "            model=model,\n",
    "            dataloader=train_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            writer=writer,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "        evaluate_epoch(\n",
    "            model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            criterion=criterion,\n",
    "            writer=writer,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    n_epochs=config[\"N_EPOCHS\"],\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate loss correct without <pad> tokens\n",
    "# TODO: add task-specific metrics\n",
    "# TODO: fix num layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "657357e3c76d50bb191e51ab294f4ff9fbef969f799c39920a46f2649b2b9e52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
